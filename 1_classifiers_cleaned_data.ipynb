{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK manual installation\n",
    "\n",
    "# pip install nltk==3.6.5\n",
    "\n",
    "# needed downgraded from 3.6.6 to 3.6.5 due to a bug in NLTK that occurs when a \n",
    "# .,? and ! are placed at the beginning of the sentence / string\n",
    "\n",
    "# in prompt set NLTK in the environment\n",
    "# set NLTK_DATA=H:\\nltk_data\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import nltk             \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# setting directory\n",
    "os.chdir('K:\\Specialemappe_XD1')\n",
    "\n",
    "# check NLTK´s operativity without the bug encountered with NLTK 3.6.6\n",
    "check_tokenizer = '.?!Den nuværende sætning er en eksempel til at teste tokenizer og at nå en potentielle bug'\n",
    "check_nltk_operativity = word_tokenize(check_tokenizer, language= 'danish')\n",
    "print(check_nltk_operativity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labelled sample\n",
    "load_sample=pd.read_excel('labelled_sample.xlsx')\n",
    "load_sample.columns\n",
    "\n",
    "# identify any NaN values\n",
    "load_sample.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing instances that are rejected by the same schooling institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting 187 instances not recognized by educational institutions or with mixed classes\n",
    "rows_to_delete = load_sample.loc[load_sample['instances_to_discard']=='delete']\n",
    "load_sample.drop(load_sample.loc[load_sample['instances_to_discard']=='delete'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dropping drop useless columns\n",
    "df = load_sample.drop(columns=['Unnamed: 0', 'index', 'instances_to_discard'])\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA IMBALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count instances per classes\n",
    "df['set_label'].value_counts()\n",
    "\n",
    "# set interested classes with integer labels \n",
    "df.loc[df['set_label'] == 'dansk', 'set_label'] = 1\n",
    "df.loc[df['set_label'] == 'idræt', 'set_label'] = 2\n",
    "df.loc[df['set_label'] == 'matematik', 'set_label'] = 3\n",
    "df.loc[df['set_label'] == 'temporary', 'set_label'] = 4\n",
    "df.loc[df['set_label'] == 'other', 'set_label'] = 0\n",
    "\n",
    "# set all other classes as the class '0'\n",
    "df['set_label'] = np.where(\n",
    "    (df['set_label'] !=1) & (df['set_label'] !=2) & (df['set_label'] !=3) & (df['set_label'] !=4), 0, df['set_label'])\n",
    "df\n",
    "\n",
    "# class imbalance \n",
    "df['set_label'].value_counts() \n",
    "\n",
    "# 0 = 43674, 84.29\n",
    "# 1 = 888, 1.71\n",
    "# 2 = 810, 1.56\n",
    "# 3 = 950, 1.83\n",
    "# 4 = 5491, 10.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - _ / Â are most likely to induce  misclassification errors \n",
    "# some numbers and § are often used by schools; \n",
    "\n",
    "df_to_clean = df\n",
    "\n",
    "# remove unicode characters and numbers from data (noise)\n",
    "char_noise = r'[-_()\\\"#@;:`.''!?*´.:;,<>=+^Â/]'\n",
    "df_to_clean['name_activity']= df_to_clean['name_activity'].apply(lambda x: re.sub(char_noise, ' ', x))\n",
    "digit_noise= r'[0123456789]'\n",
    "df_to_clean['name_activity']= df_to_clean['name_activity'].apply(lambda x: re.sub(digit_noise, '', x))\n",
    "\n",
    "# # delete artficially created double whitespaces\n",
    "df_to_clean['name_activity']= df_to_clean['name_activity'].apply(lambda x: x.replace('  ', ' '))\n",
    "# # needed a 2nd time\n",
    "df_to_clean['name_activity']= df_to_clean['name_activity'].apply(lambda x: x.replace('  ', ' '))\n",
    "\n",
    "# cleaned df \n",
    "df_cleaned = df_to_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FEATURE EXTRACTION\n",
    "\n",
    "# variable containing name activities\n",
    "content_cleaned = df_cleaned['name_activity']\n",
    "\n",
    "# text composed by the name of the activites from which word features can be obtained\n",
    "content_text_cleaned=[]\n",
    "\n",
    "# variable containing Danish stopwords\n",
    "dansk_stopwords = set(stopwords.words('danish'))\n",
    "\n",
    "for name in content_cleaned:\n",
    "    #lowercasing\n",
    "    name = name.lower()\n",
    "    # characters and numbers has been removed but remains §\n",
    "    # substitute with whitespace to make possible a better tokenization\n",
    "    \n",
    "    #tokenizing \n",
    "    name = word_tokenize(name, language= 'danish')\n",
    "    \n",
    "    for token in name:\n",
    "        if token != ' ':\n",
    "            if token != '':\n",
    "                if token not in dansk_stopwords:\n",
    "                    content_text_cleaned.append(token)\n",
    "\n",
    "print('Example of the output:')\n",
    "print(content_text_cleaned[100:150])\n",
    "\n",
    "# variable containing the most frequent words\n",
    "distr_cleaned_words = nltk.FreqDist(w for w in content_text_cleaned) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the output:\n",
    "['gård', 'mellem', 'bygn', 'lundegade', 'klyngetid', 'sammen', 'fie', 'forberede', 'prøvehandlinger', 'netværksmøder', 'naturfag', 'læ', 'vej', 'tovholder', 'bordtennis', 'badmintontræf', 'cb', 'b', 'dansk', 'lat', 'sfo', 'aftenfest', 'sfo', 'kl', 'projektopgaver', 'hjemmeundervisn', 'corona', 'x', 'x', 'f', 'd', 's', 'modul', 'b', 'ik', 'plf', 'workshop', 'årgang', 'klasselærerdag', 'gårdvagt', 'gymnastiksalen', 'overlevering', 'tysk', 'vikar', 'pauser', 'klasse', 'spis', 'gårdvagt', 'kids', 'volley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selector\n",
    "def document_features(document, word_features):\n",
    "    \n",
    "        document_words = set(document)\n",
    "        features = {}\n",
    "        for word in word_features :\n",
    "            features['contains ({})'.format(word)] = (word in document_words)\n",
    "        return features\n",
    "\n",
    "# features\n",
    "mostfq_word_cleaned= nltk.FreqDist(w for w in content_text_cleaned) \n",
    "word_features_cleaned = list(mostfq_word_cleaned)[:400]\n",
    "\n",
    "# additional RULE BASED features\n",
    "rule_features_cleaned = [\n",
    "    'matematikkens', 'geogebra', 'ing', 'svø', 'vø', 'svøm',\\\n",
    "    'atletik', 'basket','kids', 'kidsvolley', 'volley', 'motion',\\\n",
    "    'da', 'dsa', 'andet', 'andetsprog','dansk andet sprog',\\\n",
    "    'børnhave', 'basis', 'basisdansk',\\\n",
    "    'fp','pf', 'eksam',\\\n",
    "    'planlægning', '§',\\\n",
    "    'ffmat', 'ffdan', 'klassemøde', 'læringssamtale', 'fasa', 'konference',\\\n",
    "    'vej', 'vejledning', 'læsevejledere', 'matematikvejleder', 'idrv', 'matv',\\\n",
    "    'vikar', 'eks',\\\n",
    "    'skal ikke', 'kørsel', 'tur', 'studietur', 'kommunale', 'fælleskommnunal','skoleintro', 'praktik']\n",
    "\n",
    "\n",
    "word_feat_cleaned = list(itertools.chain(word_features_cleaned, rule_features_cleaned))\n",
    "len(word_feat_cleaned)\n",
    "#save features cleaned\n",
    "# with open('all_features', 'wb') as fp:\n",
    "#     pickle.dump(word_feat_cleaned, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(text,cutoffs=[0.8,0.9]):\n",
    "    train = text[:int(len(text)*cutoffs[0])]\n",
    "    val = text[int(len(text)*cutoffs[0]):int(len(text)*cutoffs[1])]\n",
    "    test = text[int(len(text)*cutoffs[1]):]\n",
    "    return train, val, test\n",
    "\n",
    "train_content, val_content, test_content = train_val_test(df_cleaned)\n",
    "train_content['set_label'].value_counts()   #  >654 per class\n",
    "val_content['set_label'].value_counts()     # >71 perr class\n",
    "test_content['set_label'].value_counts()    # >85 per class\n",
    " \n",
    "#training set building TUPLE\n",
    "train_data_tuple = (train_content['name_activity'], train_content['set_label']) #create dataframe tuple\n",
    "set_training=[]\n",
    "counter=0 #used for indexing\n",
    "for _ in train_data_tuple[0]:\n",
    "    set_training.append([(i.iloc[counter]) for i in train_data_tuple]) #append name emne and its label as a tuple\n",
    "    counter+=1\n",
    "    \n",
    "#validation set building TUPLE\n",
    "val_data_tuple = (val_content['name_activity'], val_content['set_label']) #create dataframe tuple\n",
    "set_validation=[]\n",
    "counter=0 #used for indexing\n",
    "for _ in val_data_tuple[0]:\n",
    "    set_validation.append([(i.iloc[counter]) for i in val_data_tuple]) #append name emne and its label as a tuple\n",
    "    counter+=1\n",
    "\n",
    "#test set building TUPLE\n",
    "test_data_tuple=(test_content['name_activity'], test_content['set_label']) #create dataframe tuple\n",
    "set_test=[]\n",
    "counter=0 #used for indexing\n",
    "for _ in test_data_tuple[0]:\n",
    "    set_test.append([(i.iloc[counter]) for i in test_data_tuple]) #append name emne and its label as a tuple\n",
    "    counter+=1\n",
    "\n",
    "print(\"Training tuple length:\", len(set_training))\n",
    "print(\"Validation tuple length:\", len(set_validation))\n",
    "print(\"Test tuple length:\", len(set_test))\n",
    "\n",
    "\n",
    "train_set = [(document_features(word_tokenize(w.lower(), language='danish'),word_feat_cleaned), label) \n",
    "             for w, label in set_training]# Training set - def document_feature on each string and pair with class\n",
    "\n",
    "val_set = [(document_features(word_tokenize(w.lower(), language='danish'),word_feat_cleaned), label) \n",
    "           for w, label in set_validation]# Validation set - def document_feature on each string and pair with class \n",
    "\n",
    "test_set = [(document_features(word_tokenize(w.lower(), language='danish'),word_feat_cleaned), label) \n",
    "            for w, label in set_test]# Test set - def document_feature on each string and pair with class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training tuple length: 41450  \n",
    "\n",
    "#### Validation tuple length: 5181\n",
    "#### Test tuple length: 5182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER : NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNB_cl = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# classes\n",
    "target_names = ['0','1','2', '3', '4'] \n",
    "\n",
    "# performance on validation set\n",
    "y_val = [(y) for _, y in val_set]\n",
    "pred_NBcl_val = [clfNB_cl.classify(p) for p, _ in val_set]\n",
    "f1_NBcl_val = f1_score(y_val, pred_NBcl_val, average='macro')\n",
    "print(\"Naive Bayes Classifier´s F1-Score val_set:\", round(f1_NBcl_val, 4),\"\\n\")\n",
    "confusion_matrix(y_val, pred_NBcl_val)\n",
    "# Print classification report\n",
    "print(classification_report(y_val, pred_NBcl_val, target_names=target_names))\n",
    "\n",
    "#performance on test set\n",
    "y_test = [(y) for _, y in test_set]\n",
    "pred_NBcl_test = [clfNB_cl.classify(p) for p, _ in test_set]\n",
    "f1_NBcl_test = f1_score(y_test, pred_NBcl_test, average='macro') \n",
    "print(\"Naive Bayes Classifier´s F1-Score test_set:\", round(f1_NBcl_test, 4),\"\\n\")\n",
    "confusion_matrix(y_test, pred_NBcl_test)\n",
    "# Print classification report\n",
    "print(classification_report(y_test, pred_NBcl_test, target_names=target_names))\n",
    "\n",
    "# # save model\n",
    "# with open('NB_cleaned.pickle', 'wb') as f:\n",
    "#     pickle.dump(clfNB, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_NB_cleaned= clfNB_cl.show_most_informative_features(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER : LOGISTIC CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights after grid search\n",
    "weights_log = {0: 0.44, 1: 0.56, 2: 0.56, 3: 0.56, 4:0.56}\n",
    "\n",
    "clfLOG_cl = SklearnClassifier(LogisticRegression(random_state = 10,\n",
    "                                               multi_class='ovr',\n",
    "                                               penalty='l2',\n",
    "                                               # class_weight= weights_cl\n",
    "                                               class_weight=weights_log)).train(train_set)\n",
    "\n",
    "\n",
    "# performance on validation set\n",
    "y_val = [(y) for _, y in val_set]\n",
    "pred_logcl_val = [clfLOG_cl.classify(p) for p, _ in val_set]\n",
    "f1_logcl_val = f1_score(y_val, pred_logcl_val, average='macro')\n",
    "print(\"Logistic Classifier´s F1-Score val_set:\", round(f1_logcl_val, 4),\"\\n\")\n",
    "# confusion_matrix(y_val, pred_logcl_val)\n",
    "print(classification_report(y_val, pred_logcl_val, target_names=target_names))\n",
    "\n",
    "\n",
    "# performance on test set\n",
    "y_test = [(y) for _, y in test_set]\n",
    "pred_logcl_test = [clfLOG_cl.classify(p) for p, _ in test_set]\n",
    "f1_logcl_test = f1_score(y_test, pred_logcl_test, average='macro') \n",
    "print(\"Logistic Classifier´s F1-Score test_set:\", round(f1_logcl_test, 4),\"\\n\")\n",
    "# confusion_matrix(y_test, pred_logcl_test)\n",
    "print(classification_report(y_test, pred_logcl_test, target_names=target_names))\n",
    "\n",
    "# # save model\n",
    "# with open('LOG_cleaned.pickle', 'wb') as f:\n",
    "#     pickle.dump(clfLOG, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER : SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM linear : SVC + loss = 'hinge'\n",
    "\n",
    "# weights after grid search\n",
    "svm_weights = {0: 0.11,1: 0.89, 2: 0.89,3: 0.89, 4: 0.89}\n",
    "\n",
    "clfSVM_cl = SklearnClassifier(LinearSVC(random_state=10,\n",
    "                                        C=1.8,\n",
    "                                        penalty = 'l2',\n",
    "                                        loss = 'hinge',\n",
    "                                        multi_class='ovr',\n",
    "                                        class_weight=svm_weights)).train(train_set)\n",
    "\n",
    "# performance on validation set\n",
    "y_val = [(y) for _, y in val_set]\n",
    "pred_svm_val_cl = [clfSVM_cl.classify(p) for p, _ in val_set]\n",
    "f1_svm_val = f1_score(y_val, pred_svm_val_cl, average='macro')\n",
    "print(\"SVM linear´s F1-Score val_set:\", round(f1_svm_val, 4),\"\\n\")\n",
    "# confusion_matrix(y_val, pred_svm_val)\n",
    "print(classification_report(y_val, pred_svm_val_cl, target_names=target_names))\n",
    "\n",
    "# performance on test set\n",
    "y_test = [(y) for _, y in test_set]\n",
    "pred_svm_test_cl = [clfSVM_cl.classify(p) for p, _ in test_set]\n",
    "f1_svm_test = f1_score(y_test, pred_svm_test_cl, average='macro') \n",
    "print(\"SVM linear´s F1-Score test_set:\", round(f1_svm_test, 4),\"\\n\")\n",
    "# confusion_matrix(y_test, pred_svm_test)\n",
    "print(classification_report(y_test, pred_svm_test_cl, target_names=target_names))\n",
    "\n",
    "# # save model\n",
    "# with open('SVM_cleaned.pickle', 'wb') as f:\n",
    "#     pickle.dump(clfSVM_cl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subjective evaluation on activities occurring more than 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% how many activities occurring equal or more than 500 \n",
    "\n",
    "#are within this sample? 25.15%\n",
    "\n",
    "load_act500=pd.read_excel('act_occuring_more500.xlsx')\n",
    "\n",
    "load_act500.columns\n",
    "\n",
    "df_500 = load_act500.drop(columns=['Unnamed: 0', 'initial_index'])\n",
    "df_500.columns\n",
    "\n",
    "name_500 = list(set(df_500['Aktivitet navn'].to_list()))\n",
    "print(len(name_500))\n",
    "\n",
    "df_sample = list(set(df['name_activity'].to_list()))\n",
    "\n",
    "intersection_sample = [name for name in name_500 if name in df_sample]\n",
    "print(len(intersection_sample))\n",
    "\n",
    "# how many within the training set? 19.82 %\n",
    "list_train = list(set(train_content['name_activity'].to_list()))\n",
    "intersection_train = [name for name in name_500 if name in list_train]\n",
    "print(len(intersection_train))\n",
    "\n",
    "# how many within the miclassification errors made by svm?\n",
    "# validation set \n",
    "val_set_SVM = val_content\n",
    "val_set_SVM['Predictions'] = pred_svm_val \n",
    "val_set_SVM['SVM Errors'] = val_set_SVM['Predictions'] == val_set_SVM['set_label']\n",
    "SVM_errors_val = val_set_SVM[val_set_SVM['SVM Errors'] == False]\n",
    "len(SVM_errors_val)\n",
    "\n",
    "SVM_val_list = list(set(SVM_errors_val['name_activity'].to_list()))\n",
    "svm_val_errors_intersection = [name for name in name_500 if name in SVM_val_list]\n",
    "print(len(svm_val_errors_intersection))\n",
    "\n",
    "# test set\n",
    "test_set_SVM = test_content\n",
    "test_set_SVM['Predictions'] = pred_svm_test\n",
    "test_set_SVM['SVM  Errors'] = test_set_SVM['Predictions'] == test_set_SVM['set_label']\n",
    "SVM_errors_test = test_set_SVM[test_set_SVM['SVM  Errors'] == False]\n",
    "len(SVM_errors_test)\n",
    "SVM_test_list = list(set(SVM_errors_test['name_activity'].to_list()))\n",
    "svm_test_errors_intersection = [name for name in name_500 if name in SVM_test_list]\n",
    "print(len(svm_test_errors_intersection))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
