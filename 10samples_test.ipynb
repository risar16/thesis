{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import nltk             \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load dataset properly\n",
    "df10 = pd.read_excel('10samples.xlsx')\n",
    "\n",
    "# count instances per classes\n",
    "df10['set_label'].value_counts()\n",
    "\n",
    "# set interested classes with integer labels \n",
    "df10.loc[df10['set_label'] == 'dansk', 'set_label'] = 1\n",
    "df10.loc[df10['set_label'] == 'idr√¶t', 'set_label'] = 2\n",
    "df10.loc[df10['set_label'] == 'matematik', 'set_label'] = 3\n",
    "df10.loc[df10['set_label'] == 'temporary', 'set_label'] = 4\n",
    "df10.loc[df10['set_label'] == 'other', 'set_label'] = 0\n",
    "\n",
    "# set all other classes as the class '0'\n",
    "df10['set_label'] = np.where(\n",
    "    (df10['set_label'] !=1) & (df10['set_label'] !=2) & (df10['set_label'] !=3) & (df10['set_label'] !=4), 0, df10['set_label'])\n",
    "\n",
    "# class imbalance \n",
    "df10['set_label'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% data tuple \n",
    "tuple_10samples = (df10['name_activity'], df10['set_label']) #create dataframe tuple\n",
    "set_10samples=[]\n",
    "counter=0 #used for indexing\n",
    "for _ in tuple_10samples[0]:\n",
    "    set_10samples.append([(i.iloc[counter]) for i in tuple_10samples]) #append name emne and its label as a tuple\n",
    "    counter+=1\n",
    "print(\"10samples tuple length:\", len(set_10samples))\n",
    "\n",
    "word_features_all = pickle.load(open(\"all_features_noise.pickle\", \"rb\"))\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    \n",
    "        document_words = set(document)\n",
    "        features = {}\n",
    "        for word in word_features :\n",
    "            features['contains ({})'.format(word)] = (word in document_words)\n",
    "        return features\n",
    "\n",
    "#without features\n",
    "data_to_test = [(document_features(word_tokenize(w.lower(), language='danish'),word_features_all), label) \n",
    "              for w, label in set_10samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2 classifiers NB LOG not cleaned\n",
    "NB_noise = open(\"NB_notcleaned.pickle\", \"rb\")\n",
    "LOG_noise = open(\"LOG_notcleaned.pickle\", \"rb\")\n",
    "SVM_noise= open(\"SVM_notcleaned.pickle\", \"rb\")\n",
    "\n",
    "clf_nb = pickle.load(NB_noise)\n",
    "clf_log = pickle.load(LOG_noise)\n",
    "clf_svm = pickle.load(SVM_noise)\n",
    "\n",
    "y_10 = [(y) for _, y in data_to_test]\n",
    "target_names = ['0','1','2', '3', '4'] # classes\n",
    "\n",
    "pred_nb = [clf_nb.classify(p) for p, _ in data_to_test]\n",
    "pred_log = [clf_log.classify(p) for p, _ in data_to_test]\n",
    "pred_svm = [clf_svm.classify(p) for p, _ in data_to_test]\n",
    "\n",
    "\n",
    "f1_NB = f1_score(y_10, pred_nb, average='macro')\n",
    "print(\"F1-Score NB:\", round(f1_NB, 4),\"\\n\")\n",
    "print(classification_report(y_10, pred_nb, target_names=target_names))\n",
    "\n",
    "\n",
    "f1_LOG = f1_score(y_10, pred_log, average='macro') \n",
    "print(\"F1-Score LOG:\", round(f1_LOG, 4),\"\\n\")\n",
    "print(classification_report(y_10, pred_log, target_names=target_names))\n",
    "\n",
    "\n",
    "f1_SVM = f1_score(y_10, pred_svm, average='macro') \n",
    "print(\"F1-Score SVM:\", round(f1_SVM, 4),\"\\n\")\n",
    "print(classification_report(y_10, pred_svm, target_names=target_names))\n",
    "\n",
    "\n",
    "#add results presdicitons in dataframe\n",
    "df10['NB predictions']=pred_nb\n",
    "df10['LOG predictions']=pred_log\n",
    "df10['SVM predictions']=pred_svm\n",
    "# =============================================================================\n",
    "df10.to_excel('10samples_predictions_NB_LOG_SVM.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
