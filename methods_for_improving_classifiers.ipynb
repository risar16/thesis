{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I verified in different points in time, whether the number of features, stop words and stemmed could influence the performance of the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that 400 features provided the best results in terms of F1-Macro score; stop words are not influential as well as stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#testing a) more word features may improve performance; noisy\n",
    "#testing b) no stopwords\n",
    "#testomg c) stemmed word features; noisy vs preprocessed\n",
    "\n",
    "#a) \n",
    "\n",
    "most_fq_words = nltk.FreqDist(w for w in content_text)\n",
    "word_features_experiment = list(most_fq_words)\n",
    "quantity_words = [100, 150, 200, 300, 400, 410, 420, 450, 500]\n",
    "\n",
    "results_morefeat_NB = []\n",
    "results_morefeat_LOG = []\n",
    "\n",
    "for number in quantity_words:\n",
    "    more_word_features = list(word_features_experiment)[:number]\n",
    "    \n",
    "    train_more = [(document_features(word_tokenize(w.lower(), language='danish'),word_features), label) for w, label in set_training]\n",
    "    val_more = [(document_features(word_tokenize(w.lower(), language='danish'),word_features), label) for w, label in set_validation]\n",
    "    test_more = [(document_features(word_tokenize(w.lower(), language='danish'),word_features), label) for w, label in set_test]\n",
    "    \n",
    "    clfNB_more = nltk.NaiveBayesClassifier.train(train_more)\n",
    "    pred_NB_more = [clfNB_more.classify(p) for p, _ in val_set]\n",
    "    \n",
    "    clfLOG_more = SklearnClassifier(LogisticRegression(random_state = 10,multi_class='ovr',\n",
    "                                                        penalty='l2',# class_weight= weights_cl\n",
    "                                                        class_weight=weights_manual)).train(train_set)\n",
    "    pred_LOG_more = [clfLOG_more.classify(p) for p, _ in val_set]\n",
    "    \n",
    "    y_val = [(y) for _, y in val_set]\n",
    "    f1_NB_more = f1_score(y_val, pred_NB_more, average='macro')\n",
    "    f1_LOG_more = f1_score(y_val, pred_LOG_more, average='macro')\n",
    "\n",
    "    results_morefeat_NB.append([number, f1_NB_more])\n",
    "    results_morefeat_LOG.append([number, f1_LOG_more])\n",
    "    \n",
    "results_morefeat_NB = pd.DataFrame(results_morefeat_NB, columns = [\"Number of most occurring words-features\", \"F1-Score of Naive Bayes\"])\n",
    "results_morefeat_LOG = pd.DataFrame(results_morefeat_LOG, columns = [\"Number of most occurring words-features\", \"F1-Score of Logistic\"])\n",
    "results_morefeat_NB \n",
    "results_morefeat_LOG\n",
    "\n",
    "\n",
    "# b) Naive Bayes\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "dansk_stopwords = set(stopwords.words('danish'))\n",
    "content_no_stopwords = []\n",
    "for name in content:\n",
    "    name = name.lower()\n",
    "    # name = name.replace(\".\", \"\")\n",
    "    name = word_tokenize(name, language='danish')\n",
    "    for token in name:\n",
    "        if token != \"\":\n",
    "            if token not in dansk_stopwords:\n",
    "                content_no_stopwords.append(token)\n",
    "content_no_stopwords[:10]\n",
    "\n",
    "without_stopwords = nltk.FreqDist(t.lower() for t in content_no_stopwords)\n",
    "n_words = [100, 150, 200, 300, 400, 410, 420, 450, 500]\n",
    "\n",
    "results_no_stopwords = []\n",
    "results_nostpw_log = []\n",
    "n_words = [100, 150, 200, 300, 400, 410, 420, 450, 500]\n",
    "for number in n_words:\n",
    "    nostpw_features = list(without_stopwords)[:number]\n",
    "    \n",
    "    train_nostpw = [(document_features(word_tokenize(w.lower(), language='danish'),nostpw_features), label) for w, label in set_training]\n",
    "    val_nostpw = [(document_features(word_tokenize(w.lower(), language='danish'),nostpw_features), label) for w, label in set_validation]\n",
    "    test_nostpw = [(document_features(word_tokenize(w.lower(), language='danish'),nostpw_features), label) for w, label in set_test]\n",
    "    \n",
    "    clfNB_nostpw = nltk.NaiveBayesClassifier.train(train_nostpw)\n",
    "    pred_nostpw = [clfNB_nostpw.classify(p) for p, _ in val_nostpw]\n",
    "    clfNB_nostop_fscore = f1_score(y_val, pred_nostpw, average='macro')\n",
    "    \n",
    "    log_clf_nostpw = SklearnClassifier(LogisticRegression(random_state = 10,multi_class='ovr',penalty='l2',class_weight=weights_manual)).train(train_nostpw)\n",
    "    pred_log_nostpw = [log_clf_nostpw.classify(p) for p, _ in val_nostpw]\n",
    "    log_nostpw_f1 = f1_score(y_val, pred_log_nostpw, average='macro')\n",
    "    \n",
    "    results_nostpw_log.append([number, log_nostpw_f1])\n",
    "    results_no_stopwords.append([number, clfNB_nostop_fscore])\n",
    "\n",
    "results_nostpw_log = pd.DataFrame(results_nostpw_log, columns = [\"Number words-features no stopwords\", \"F1-Score of LOG\"])\n",
    "results_no_stopwords = pd.DataFrame(results_no_stopwords, columns = [\"Number words-features no stopwords\", \"F1-Score of classifier\"])\n",
    "results_no_stopwords\n",
    "results_nostpw_log\n",
    "\n",
    "\n",
    "\n",
    "# c) \n",
    "\n",
    "stemmer = SnowballStemmer('danish')\n",
    "stemmed = [stemmer.stem(word) for word in content_text]\n",
    "stemmed_words = nltk.FreqDist(w.lower() for w in stemmed)\n",
    "\n",
    "results_NB_stemmed = []\n",
    "results_LOG_stemmed = []\n",
    "n_words = [100, 150, 200, 300, 400, 410, 420, 450, 500]\n",
    "for number in n_words:\n",
    "    words_features_stemmed = list(stemmed_words)[:number]\n",
    "    \n",
    "    train_stemmed = [(document_features(word_tokenize(w.lower(), language='danish'),words_features_stemmed), label) for w, label in set_training]\n",
    "    val_stemmed = [(document_features(word_tokenize(w.lower(), language='danish'),words_features_stemmed), label) for w, label in set_validation]\n",
    "    test_stemmed = [(document_features(word_tokenize(w.lower(), language='danish'),words_features_stemmed), label) for w, label in set_test]\n",
    "    \n",
    "    clfNB_stemmed = nltk.NaiveBayesClassifier.train(train_stemmed)\n",
    "    pred_NB_stemmed = [clfNB_stemmed.classify(p) for p, _ in val_stemmed]\n",
    "    clfNB_stemmed_fscore = f1_score(y_val, pred_NB_stemmed, average='macro')\n",
    "    \n",
    "    clfLog_stemmed = SklearnClassifier(LogisticRegression(random_state = 10,multi_class='ovr',penalty='l2',class_weight=weights_manual)).train(train_stemmed)\n",
    "    pred_log_stemmed = [clfLog_stemmed.classify(p) for p, _ in val_stemmed]\n",
    "    log_stemmed_f1 = f1_score(y_val, pred_log_stemmed, average='macro')\n",
    "\n",
    "    results_NB_stemmed.append([number, clfNB_stemmed_fscore])\n",
    "    results_LOG_stemmed.append([number, log_stemmed_f1])\n",
    "\n",
    "results_NB_stemmed = pd.DataFrame(results_NB_stemmed, columns = [\"Number stemmed words-features\", \"F1-Score of NB\"])\n",
    "results_LOG_stemmed = pd.DataFrame(results_LOG_stemmed, columns = [\"Number stemmed words-features\", \"F1-Score of LOG\"])\n",
    "results_NB_stemmed\n",
    "results_LOG_stemmed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
